{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "from scipy.interpolate import make_interp_spline\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import os\n",
    "import joblib\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, LayerNormalization, MultiHeadAttention, Dropout, Layer, Flatten, GlobalAveragePooling1D\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "id": "7af8c691d8e34830"
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Constants\n",
    "mu_0 = 4 * np.pi * 1e-7  # Permeability of free space (H/m)\n",
    "frequencies = np.array([1.000000e+04, 8.799998e+03, 7.200000e+03, 6.000000e+03, 5.200001e+03,  4.400000e+03, 3.600000e+03, 3.000001e+03, 2.600000e+03, 2.200000e+03,  1.800000e+03, 1.500000e+03, 1.300000e+03, 1.058824e+03, 9.176473e+02,  7.764706e+02, 6.352943e+02, 5.294117e+02, 4.588235e+02, 3.882354e+02,  3.176470e+02, 2.647059e+02, 2.294118e+02, 1.941176e+02, 1.588235e+02,  1.323529e+02, 1.147059e+02, 9.705883e+01, 7.941176e+01, 6.499999e+01,  5.499999e+01, 3.750000e+01, 3.250000e+01, 2.750000e+01, 2.250000e+01,  1.875000e+01, 1.625000e+01, 1.375000e+01, 1.125000e+01, 9.375000e+00,  8.125000e+00, 6.875000e+00, 5.625000e+00, 4.687500e+00, 4.062500e+00,  3.437500e+00, 2.812500e+00, 2.343750e+00, 1.718750e+00, 1.406250e+00,  1.171875e+00, 1.015625e+00, 8.593750e-01, 7.031250e-01, 5.859375e-01,  5.078125e-01, 4.296875e-01, 3.515625e-01, 2.929688e-01, 2.539063e-01,  1.757813e-01, 8.789063e-02, 5.371094e-02, 4.394531e-02, 3.662109e-02,  3.173828e-02, 2.685547e-02, 2.197266e-02, 1.831055e-02, 1.586914e-02,  1.342773e-02, 1.098633e-02, 9.155274e-03, 7.934572e-03, 6.713867e-03,  5.493165e-03, 4.577638e-03, 3.967284e-03, 3.356934e-03, 2.746581e-03,  2.288818e-03, 1.983643e-03, 1.678467e-03, 1.373291e-03, 1.144409e-03,  9.918214e-04, 8.392333e-04, 6.866456e-04, 5.722047e-04, 3.433228e-04])\n",
    "periods = 1 / frequencies  # Periods in seconds\n",
    "resistivity_range = [1, 500]  # Min and max resistivities in Ohm.m\n",
    "rho_ref = 50  # Reference resistivity in Ohm.m\n",
    "num_layers = 50\n",
    "\n",
    "# Generate depths using an initial value and multiplying by a factor\n",
    "depths = [8]  # Starting depth\n",
    "for i in range(num_layers - 1):\n",
    "    depth = depths[-1] * 1.2078  #We want to get 50 depth points ending at about 100,000\n",
    "    depths.append(depth)\n",
    "\n",
    "\n",
    "# Calculate layer thicknesses from depths\n",
    "layer_thicknesses = np.diff(depths)  # Difference between consecutive depths\n",
    "layer_thicknesses = np.insert(layer_thicknesses, 0, depths[0])  # Insert first depth as the initial thickness\n",
    "\n",
    "\n",
    "# Function to generate resistivity profile based on depths\n",
    "def generate_smooth_resistivity_profile(depths, resistivity_range, num_spline_points, smoothing=True):\n",
    "    \"\"\"\n",
    "    Generates a smooth resistivity profile where resistivities align with depth points.\n",
    "\n",
    "    Parameters:\n",
    "    - depths: Array of depth points for the layers\n",
    "    - resistivity_range: List with min and max resistivity values\n",
    "    - num_spline_points: Number of points to generate spline (int)\n",
    "    - smoothing: Whether to apply Gaussian smoothing (bool)\n",
    "\n",
    "    Returns:\n",
    "    - resistivities: Array of resistivity values aligned with each layer's depth\n",
    "    \"\"\"\n",
    "    # Generate random spline points between the minimum and maximum depths\n",
    "    spline_points_depth = np.linspace(depths[0], depths[-1], num=num_spline_points)\n",
    "    resistivity_points = np.random.uniform(resistivity_range[0], resistivity_range[1], num_spline_points)\n",
    "\n",
    "    # Create cubic spline for interpolation\n",
    "    spline = make_interp_spline(spline_points_depth, resistivity_points, k=3)\n",
    "\n",
    "    # Interpolate resistivities for the given depths\n",
    "    resistivities = spline(depths)\n",
    "\n",
    "    if smoothing:\n",
    "        # Apply Gaussian smoothing to resistivities to reduce sharp transitions\n",
    "        resistivities = gaussian_filter1d(resistivities, sigma=1)\n",
    "\n",
    "    return resistivities\n",
    "\n",
    "\n",
    "# Compute apparent resistivity and phase\n",
    "def compute_apparent_resistivity_and_phase(thicknesses, conductivities, periods):\n",
    "    apparent_resistivity = []\n",
    "    phase = []\n",
    "\n",
    "    for T in periods:\n",
    "        omega = 2 * np.pi / T  # Angular frequency (rad/s)\n",
    "        cns = np.zeros(len(conductivities), dtype=complex)\n",
    "        cns[-1] = 1 / np.sqrt(mu_0 * omega * conductivities[-1] * 1j)\n",
    "\n",
    "        for j in reversed(range(len(thicknesses))):\n",
    "            K = np.sqrt(conductivities[j] * mu_0 * omega * 1j)\n",
    "            layer_thickness = thicknesses[j] if j < len(thicknesses) - 1 else np.inf\n",
    "            if j + 1 < len(cns):\n",
    "                cns[j] = (1 / K) * ((K * cns[j + 1] + np.tanh(K * layer_thickness)) /\n",
    "                                    (1 + K * cns[j + 1] * np.tanh(K * layer_thickness)))\n",
    "\n",
    "        Z = cns[0]\n",
    "        rho_apparent = np.abs(Z) ** 2 / (mu_0 * omega)\n",
    "        phi = np.angle(Z, deg=True) + 90\n",
    "\n",
    "        apparent_resistivity.append(rho_apparent)\n",
    "        phase.append(phi)\n",
    "\n",
    "    return np.array(apparent_resistivity), np.array(phase)\n",
    "\n",
    "\n",
    "\n",
    "num_spline_points_list = [6, 7, 8, 9, 10]  # Number of spline points\n",
    "examples_per_spline = 20000  # Number of examples per spline point count\n",
    "total_examples = examples_per_spline * len(num_spline_points_list)  # Total examples: 100,000\n",
    "\n",
    "X_model = np.empty((total_examples, num_layers))\n",
    "y_rho = np.empty((total_examples, len(periods)))\n",
    "y_phi = np.empty((total_examples, len(periods)))\n",
    "\n",
    "example_idx = 0  # Index to keep track of the current example\n",
    "\n",
    "#Generate data\n",
    "for num_spline_points in num_spline_points_list:\n",
    "    print(f\"Generating {examples_per_spline} examples with {num_spline_points} spline points...\")\n",
    "    for _ in range(examples_per_spline):\n",
    "        # Generate smooth resistivity profile\n",
    "        resistivities = generate_smooth_resistivity_profile(\n",
    "            depths=depths,\n",
    "            resistivity_range=resistivity_range,\n",
    "            num_spline_points=num_spline_points,\n",
    "            smoothing=True\n",
    "        )\n",
    "        conductivities = 1 / resistivities\n",
    "        apparent_resistivity, phase = compute_apparent_resistivity_and_phase(\n",
    "            layer_thicknesses,\n",
    "            conductivities,\n",
    "            periods\n",
    "        )\n",
    "\n",
    "        # Store the results\n",
    "        X_model[example_idx] = resistivities\n",
    "        y_rho[example_idx] = apparent_resistivity\n",
    "        y_phi[example_idx] = phase\n",
    "        example_idx += 1\n",
    "\n",
    "    print(f\"Completed {examples_per_spline} examples with {num_spline_points} spline points.\")\n",
    "\n",
    "print(\"Data generation completed.\")\n",
    "\n",
    "X_model = np.array(X_model)\n",
    "y_rho = np.array(y_rho)\n",
    "y_phi = np.array(y_phi)\n",
    "\n",
    "# Normalize data\n",
    "scaler_X_model = MinMaxScaler()\n",
    "scaler_y_rho = MinMaxScaler()\n",
    "scaler_y_phi = MinMaxScaler()\n",
    "\n",
    "scaler_X_model.fit(X_model)\n",
    "scaler_y_rho.fit(y_rho)\n",
    "scaler_y_phi.fit(y_phi)\n",
    "X_model_scaled = scaler_X_model.transform(X_model)\n",
    "y_rho_scaled = scaler_y_rho.transform(y_rho)\n",
    "y_phi_scaled = scaler_y_phi.transform(y_phi)\n",
    "\n",
    "# Combine y_rho and y_phi for the input data (each frequency has rho and phi as components)\n",
    "X_combined = np.stack((y_rho_scaled, y_phi_scaled), axis=-1)  # Shape: (num_stations, num_periods, 2)\n",
    "\n",
    "save_path = \"\"#Tha path to save the data\n",
    "\n",
    "# Save the generated data to Google Drive\n",
    "np.save(save_path + 'X_combined_u.npy', X_combined)\n",
    "np.save(save_path + 'X_model_scaled_u.npy', X_model_scaled)\n",
    "\n",
    "# Save the scalers\n",
    "joblib.dump(scaler_X_model, save_path + 'scaler_X_model_u.pkl')\n",
    "joblib.dump(scaler_y_rho, save_path + 'scaler_y_rho_u.pkl')\n",
    "joblib.dump(scaler_y_phi, save_path + 'scaler_y_phi_u.pkl')"
   ],
   "id": "initial_id"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "load_path = \"\" #Path to where the data is saved\n",
    "X_combined = np.load(os.path.join(load_path, 'X_combined2.npy'))  # Shape: (100000, 90, 2)\n",
    "X_model_scaled = np.load(os.path.join(load_path, 'X_model_scaled2.npy'))  # Shape: (100000, 100)\n",
    "print(\"Data loaded successfully.\")\n",
    "\n",
    "\n",
    "class TransformerAttentionBlock(Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, dropout_rate=0.01):\n",
    "        super(TransformerAttentionBlock, self).__init__()\n",
    "        self.attention = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            Dense(ff_dim, activation=\"relu\"),  # Feedforward network\n",
    "            Dense(embed_dim)\n",
    "        ])\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(dropout_rate)\n",
    "        self.dropout2 = Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        # Self-attention\n",
    "        attn_output = self.attention(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "\n",
    "        # Feedforward network\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "\n",
    "# Graph Attention Layer\n",
    "class GraphAttentionLayer(Layer):\n",
    "    def __init__(self, output_dim, num_heads=4, concat=False, dropout_rate=0.01, activation=None, **kwargs):\n",
    "        super(GraphAttentionLayer, self).__init__(**kwargs)\n",
    "        self.output_dim = output_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.concat = concat\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.activation = activation\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        input_dim = input_shape[0][-1]\n",
    "        self.W = self.add_weight(shape=(input_dim, self.num_heads * self.output_dim),\n",
    "                                 initializer='glorot_uniform', name='W')\n",
    "        self.a_src = self.add_weight(shape=(self.num_heads, self.output_dim, 1),\n",
    "                                     initializer='glorot_uniform', name='a_src')\n",
    "        self.a_dst = self.add_weight(shape=(self.num_heads, self.output_dim, 1),\n",
    "                                     initializer='glorot_uniform', name='a_dst')\n",
    "        super(GraphAttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        X, adjacency = inputs\n",
    "        batch_size = tf.shape(X)[0]\n",
    "        num_nodes = tf.shape(X)[1]\n",
    "\n",
    "        XW = tf.matmul(X, self.W)\n",
    "        XW = tf.reshape(XW, (batch_size, num_nodes, self.num_heads, self.output_dim))\n",
    "        XW = tf.transpose(XW, perm=[0, 2, 1, 3])\n",
    "\n",
    "        e_src = tf.einsum('hdf,bhnd->bhn', self.a_src, XW)\n",
    "        e_dst = tf.einsum('hdf,bhnd->bhn', self.a_dst, XW)\n",
    "\n",
    "        e_src_expanded = tf.expand_dims(e_src, axis=-1)\n",
    "        e_dst_expanded = tf.expand_dims(e_dst, axis=2)\n",
    "        e = e_src_expanded + e_dst_expanded\n",
    "\n",
    "        e = tf.nn.leaky_relu(e, alpha=0.2)\n",
    "        mask = adjacency > 0\n",
    "        mask = tf.expand_dims(mask, axis=1)\n",
    "        e = tf.where(mask, e, tf.fill(tf.shape(e), -1e9))\n",
    "\n",
    "        alpha = tf.nn.softmax(e, axis=-1)\n",
    "        alpha = tf.nn.dropout(alpha, rate=self.dropout_rate) if training else alpha\n",
    "        out = tf.matmul(alpha, XW)\n",
    "\n",
    "        if self.concat:\n",
    "            out = tf.transpose(out, perm=[0, 2, 1, 3])\n",
    "            out = tf.reshape(out, (batch_size, num_nodes, self.num_heads * self.output_dim))\n",
    "        else:\n",
    "            out = tf.reduce_mean(out, axis=1)\n",
    "\n",
    "        if self.activation:\n",
    "            out = self.activation(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "# GAT Model with Two Attention Blocks and Dense Layers Afterward\n",
    "class GAT(Model):\n",
    "    def __init__(self, num_heads=8, hidden_dim=64, output_dim=105, dropout_rate=0.1, **kwargs):\n",
    "        super(GAT, self).__init__(**kwargs)\n",
    "        self.gat_output_dim = output_dim\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.gat1 = GraphAttentionLayer(output_dim=hidden_dim, num_heads=num_heads, concat=True,\n",
    "                                        dropout_rate=dropout_rate, activation=tf.nn.elu)\n",
    "        self.gat2 = GraphAttentionLayer(output_dim=hidden_dim, num_heads=1, concat=False,\n",
    "                                        dropout_rate=dropout_rate, activation=tf.nn.elu)\n",
    "        # Global Pooling layer for graph-level output\n",
    "        self.global_pool = GlobalAveragePooling1D()\n",
    "\n",
    "        # Instantiate two TransformerAttentionBlocks\n",
    "        self.transformer_attention1 = TransformerAttentionBlock(embed_dim=16, num_heads=4, ff_dim=64, dropout_rate=0.01)\n",
    "        self.transformer_attention2 = TransformerAttentionBlock(embed_dim=16, num_heads=4, ff_dim=64, dropout_rate=0.01)\n",
    "\n",
    "        # Define dense layers after attention blocks\n",
    "        self.dense1 = Dense(1024)\n",
    "        self.dense2 = Dense(1024)\n",
    "        self.final_dense = Dense(output_dim)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        X, adjacency = inputs\n",
    "        X = tf.nn.dropout(X, rate=self.dropout_rate) if training else X\n",
    "        X = self.gat1((X, adjacency), training=training)\n",
    "        X = tf.nn.dropout(X, rate=self.dropout_rate) if training else X\n",
    "        X = self.gat2((X, adjacency), training=training)\n",
    "        X = self.global_pool(X)  # Shape: (batch_size, hidden_dim)\n",
    "\n",
    "        # Reshape for attention blocks\n",
    "        reshaped_output = tf.reshape(X, (-1, 4, 16))  # Since hidden_dim=64, 4*16=64\n",
    "\n",
    "        # Apply first attention block\n",
    "        attention_output = self.transformer_attention1(reshaped_output, training=training)\n",
    "\n",
    "        # Apply second attention block\n",
    "        attention_output = self.transformer_attention2(attention_output, training=training)\n",
    "\n",
    "        # Aggregate attention outputs\n",
    "        aggregated_output = tf.reduce_mean(attention_output, axis=1)  # Shape: (batch_size, embed_dim)\n",
    "\n",
    "        # Apply dense layers\n",
    "        X = self.dense1(aggregated_output)\n",
    "        X = self.dense2(X)\n",
    "        # Final output\n",
    "        X = self.final_dense(X)\n",
    "        return X\n",
    "\n",
    "# Adjacency Matrix\n",
    "def create_adjacency_matrix(num_nodes=90, k=5):\n",
    "    adj = np.zeros((num_nodes, num_nodes), dtype=np.float32)\n",
    "    for i in range(num_nodes):\n",
    "        for j in range(max(0, i - k), min(num_nodes, i + k + 1)):\n",
    "            if i != j:\n",
    "                adj[i, j] = 1.0\n",
    "    np.fill_diagonal(adj, 1.0)\n",
    "    return adj\n",
    "\n",
    "adjacency_matrix = create_adjacency_matrix(num_nodes=90, k=5)\n",
    "\n",
    "# Prepare Data\n",
    "X = X_combined  # Node features\n",
    "y = X_model_scaled  # Targets (graph-level outputs)\n",
    "\n",
    "# Replicate adjacency matrix for all samples\n",
    "num_samples = X.shape[0]\n",
    "A = np.tile(adjacency_matrix, (num_samples, 1, 1))\n",
    "\n",
    "# Split Dataset into Training and Validation Sets\n",
    "X_train, X_val, A_train, A_val, y_train, y_val = train_test_split(\n",
    "    X, A, y, test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "# Convert to TensorFlow Tensors\n",
    "X_train = tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
    "X_val = tf.convert_to_tensor(X_val, dtype=tf.float32)\n",
    "A_train = tf.convert_to_tensor(A_train, dtype=tf.float32)\n",
    "A_val = tf.convert_to_tensor(A_val, dtype=tf.float32)\n",
    "y_train = tf.convert_to_tensor(y_train, dtype=tf.float32)\n",
    "y_val = tf.convert_to_tensor(y_val, dtype=tf.float32)\n",
    "\n",
    "buffer_size = X_train.shape[0]\n",
    "\n",
    "# Create TensorFlow datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(((X_train, A_train), y_train)).shuffle(buffer_size).batch(64).prefetch(tf.data.AUTOTUNE)\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices(((X_val, A_val), y_val)).batch(64).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "print(\"Training and validation datasets prepared.\")\n",
    "\n",
    "# Define and Train the GAT Model\n",
    "gat_model = GAT(num_heads=8, hidden_dim=64, output_dim=100, dropout_rate=0.01)\n",
    "\n",
    "# Compile the Model\n",
    "gat_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "    loss='mean_squared_error',\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "# Build the Model\n",
    "node_features_input = Input(shape=(90, 2), dtype=tf.float32)\n",
    "adjacency_input = Input(shape=(90, 90), dtype=tf.float32)\n",
    "gat_output = gat_model((node_features_input, adjacency_input))\n",
    "\n",
    "# Train the Model\n",
    "history = gat_model.fit(train_dataset, validation_data=val_dataset, epochs=100)\n"
   ],
   "id": "2cea50a0c7964be0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
