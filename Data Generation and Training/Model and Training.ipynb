{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Model Training Setup for Magnetotelluric Inversion\n",
    "\n",
    "This script sets up a **deep learning model** for **MT inversion** using TensorFlow/Keras and Scikit-learn for preprocessing.\n",
    "\n",
    "## **Libraries Used**:\n",
    "- **Numerical and Scientific**: `numpy`, `scipy` for data manipulation and smoothing.\n",
    "- **Preprocessing**: `MinMaxScaler` for normalizing data, `train_test_split` for data splitting.\n",
    "- **Modeling**: Keras layers like `Dense`, `Dropout`, `MultiHeadAttention` for building the neural network.\n",
    "- **Utilities**: `joblib` for saving scalers, `os` for file handling.\n",
    "\n"
   ],
   "id": "52b75b5ed9fd059"
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "from scipy.interpolate import make_interp_spline\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import os\n",
    "import joblib\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, LayerNormalization, MultiHeadAttention, Dropout, Layer, Flatten, GlobalAveragePooling1D\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ],
   "id": "initial_id"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Data Loading for Model Training\n",
    "\n",
    "This script loads pre-saved data for model training.\n",
    "\n",
    "## **Data Files Loaded**:\n",
    "- `X_combined2.npy`: Combined apparent resistivity and phase responses (Shape: `(100000, 90, 2)`).\n",
    "- `X_model_scaled2.npy`: Normalized resistivity profiles (Shape: `(100000, 100)`).\n",
    "\n",
    "The data is loaded from the specified `load_path` and printed as a confirmation message.\n"
   ],
   "id": "6276afd423e6ecf4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "load_path = \"\" #Path to where the data is saved\n",
    "X_combined = np.load(os.path.join(load_path, 'X_combined2.npy'))  # Shape: (100000, 90, 2)\n",
    "X_model_scaled = np.load(os.path.join(load_path, 'X_model_scaled2.npy'))  # Shape: (100000, 100)\n",
    "print(\"Data loaded successfully.\")"
   ],
   "id": "f89f486316b1613d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Transformer Attention Block\n",
    "\n",
    "This class defines a **Transformer-based attention block** for deep learning models, implementing self-attention and feedforward layers.\n",
    "\n",
    "## **Components**:\n",
    "- **Self-attention**: Uses `MultiHeadAttention` to capture dependencies in input data.\n",
    "- **Feedforward Network (FFN)**: A two-layer fully connected network with ReLU activation.\n",
    "- **Layer Normalization**: Normalizes outputs to stabilize training.\n",
    "- **Dropout**: Applied after attention and FFN to prevent overfitting.\n",
    "\n",
    "## **Forward Pass**:\n",
    "1. Computes self-attention.\n",
    "2. Applies layer normalization.\n",
    "3. Passes through a feedforward network.\n",
    "4. Applies dropout and final normalization."
   ],
   "id": "a3ab8a0bc4c0357b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class TransformerAttentionBlock(Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, dropout_rate=0.01):\n",
    "        super(TransformerAttentionBlock, self).__init__()\n",
    "        self.attention = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            Dense(ff_dim, activation=\"relu\"),  # Feedforward network\n",
    "            Dense(embed_dim)\n",
    "        ])\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(dropout_rate)\n",
    "        self.dropout2 = Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        # Self-attention\n",
    "        attn_output = self.attention(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "\n",
    "        # Feedforward network\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "\n"
   ],
   "id": "8f4740a69e8cd606"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Graph Attention Layer\n",
    "\n",
    "This class implements a **Graph Attention Layer (GAT)**, which applies attention mechanisms to graph-structured data.\n",
    "\n",
    "## **Components**:\n",
    "- **Weights**: Learnable weight matrices for input features and attention scores.\n",
    "- **Attention Mechanism**: Computes attention scores for each node pair in the graph using **LeakyReLU** activation.\n",
    "- **Masking**: Masks out non-existent edges using the adjacency matrix.\n",
    "- **Softmax**: Normalizes attention scores across neighboring nodes.\n",
    "- **Dropout**: Applied to attention scores during training to prevent overfitting.\n",
    "\n",
    "## **Forward Pass**:\n",
    "1. Computes node feature transformation (`XW`).\n",
    "2. Calculates attention scores (`e_src` and `e_dst`).\n",
    "3. Applies softmax and dropout.\n",
    "4. Aggregates the output either by concatenation or averaging.\n",
    "\n",
    "This layer is used to enhance the learning of node relationships in graph neural networks.\n"
   ],
   "id": "466bc5ce7d22068"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Graph Attention Layer\n",
    "class GraphAttentionLayer(Layer):\n",
    "    def __init__(self, output_dim, num_heads=4, concat=False, dropout_rate=0.01, activation=None, **kwargs):\n",
    "        super(GraphAttentionLayer, self).__init__(**kwargs)\n",
    "        self.output_dim = output_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.concat = concat\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.activation = activation\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        input_dim = input_shape[0][-1]\n",
    "        self.W = self.add_weight(shape=(input_dim, self.num_heads * self.output_dim),\n",
    "                                 initializer='glorot_uniform', name='W')\n",
    "        self.a_src = self.add_weight(shape=(self.num_heads, self.output_dim, 1),\n",
    "                                     initializer='glorot_uniform', name='a_src')\n",
    "        self.a_dst = self.add_weight(shape=(self.num_heads, self.output_dim, 1),\n",
    "                                     initializer='glorot_uniform', name='a_dst')\n",
    "        super(GraphAttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        X, adjacency = inputs\n",
    "        batch_size = tf.shape(X)[0]\n",
    "        num_nodes = tf.shape(X)[1]\n",
    "\n",
    "        XW = tf.matmul(X, self.W)\n",
    "        XW = tf.reshape(XW, (batch_size, num_nodes, self.num_heads, self.output_dim))\n",
    "        XW = tf.transpose(XW, perm=[0, 2, 1, 3])\n",
    "\n",
    "        e_src = tf.einsum('hdf,bhnd->bhn', self.a_src, XW)\n",
    "        e_dst = tf.einsum('hdf,bhnd->bhn', self.a_dst, XW)\n",
    "\n",
    "        e_src_expanded = tf.expand_dims(e_src, axis=-1)\n",
    "        e_dst_expanded = tf.expand_dims(e_dst, axis=2)\n",
    "        e = e_src_expanded + e_dst_expanded\n",
    "\n",
    "        e = tf.nn.leaky_relu(e, alpha=0.2)\n",
    "        mask = adjacency > 0\n",
    "        mask = tf.expand_dims(mask, axis=1)\n",
    "        e = tf.where(mask, e, tf.fill(tf.shape(e), -1e9))\n",
    "\n",
    "        alpha = tf.nn.softmax(e, axis=-1)\n",
    "        alpha = tf.nn.dropout(alpha, rate=self.dropout_rate) if training else alpha\n",
    "        out = tf.matmul(alpha, XW)\n",
    "\n",
    "        if self.concat:\n",
    "            out = tf.transpose(out, perm=[0, 2, 1, 3])\n",
    "            out = tf.reshape(out, (batch_size, num_nodes, self.num_heads * self.output_dim))\n",
    "        else:\n",
    "            out = tf.reduce_mean(out, axis=1)\n",
    "\n",
    "        if self.activation:\n",
    "            out = self.activation(out)\n",
    "        return out\n",
    "\n",
    "\n"
   ],
   "id": "365c53754dba80f6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# GAT Model with Attention Blocks and Dense Layers\n",
    "\n",
    "This class defines a **Graph Attention Network (GAT)** with two attention blocks followed by dense layers for prediction.\n",
    "\n",
    "## **Components**:\n",
    "1. **Graph Attention Layers**:\n",
    "   - First GAT layer (`gat1`) with multi-head attention and concatenation.\n",
    "   - Second GAT layer (`gat2`) with single-head attention and averaging.\n",
    "2. **Transformer Attention Blocks**:\n",
    "   - Two **TransformerAttentionBlock** layers for capturing sequential dependencies.\n",
    "3. **Dense Layers**:\n",
    "   - Two dense layers with 1024 units for final feature transformation.\n",
    "   - Final dense layer outputs the predictions with the specified `output_dim`.\n",
    "4. **Global Pooling**:\n",
    "   - `GlobalAveragePooling1D` used to aggregate graph-level features.\n",
    "\n",
    "## **Forward Pass**:\n",
    "1. Passes input through GAT layers with dropout.\n",
    "2. Applies transformer attention blocks.\n",
    "3. Reduces the attention output and applies dense layers.\n",
    "4. Returns final prediction after dense transformations.\n",
    "\n",
    "## **Adjacency Matrix**:\n",
    "- Generates a **local adjacency matrix** based on node connectivity.\n"
   ],
   "id": "dfd7e8510fde7e3a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "class GAT(Model):\n",
    "    def __init__(self, num_heads=8, hidden_dim=64, output_dim=105, dropout_rate=0.1, **kwargs):\n",
    "        super(GAT, self).__init__(**kwargs)\n",
    "        self.gat_output_dim = output_dim\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.gat1 = GraphAttentionLayer(output_dim=hidden_dim, num_heads=num_heads, concat=True,\n",
    "                                        dropout_rate=dropout_rate, activation=tf.nn.elu)\n",
    "        self.gat2 = GraphAttentionLayer(output_dim=hidden_dim, num_heads=1, concat=False,\n",
    "                                        dropout_rate=dropout_rate, activation=tf.nn.elu)\n",
    "        self.global_pool = GlobalAveragePooling1D()\n",
    "\n",
    "        self.transformer_attention1 = TransformerAttentionBlock(embed_dim=16, num_heads=4, ff_dim=64, dropout_rate=0.01)\n",
    "        self.transformer_attention2 = TransformerAttentionBlock(embed_dim=16, num_heads=4, ff_dim=64, dropout_rate=0.01)\n",
    "\n",
    "        self.dense1 = Dense(1024)\n",
    "        self.dense2 = Dense(1024)\n",
    "        self.final_dense = Dense(output_dim)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        X, adjacency = inputs\n",
    "        X = tf.nn.dropout(X, rate=self.dropout_rate) if training else X\n",
    "        X = self.gat1((X, adjacency), training=training)\n",
    "        X = tf.nn.dropout(X, rate=self.dropout_rate) if training else X\n",
    "        X = self.gat2((X, adjacency), training=training)\n",
    "        X = self.global_pool(X)\n",
    "\n",
    "        # Reshape for attention blocks\n",
    "        reshaped_output = tf.reshape(X, (-1, 4, 16))\n",
    "\n",
    "        # first attention block\n",
    "        attention_output = self.transformer_attention1(reshaped_output, training=training)\n",
    "\n",
    "        # second attention block\n",
    "        attention_output = self.transformer_attention2(attention_output, training=training)\n",
    "\n",
    "        # Aggregate attention outputs\n",
    "        aggregated_output = tf.reduce_mean(attention_output, axis=1)  # Shape: (batch_size, embed_dim)\n",
    "\n",
    "        # dense layers\n",
    "        X = self.dense1(aggregated_output)\n",
    "        X = self.dense2(X)\n",
    "        # Final output\n",
    "        X = self.final_dense(X)\n",
    "        return X\n",
    "\n",
    "# Adjacency Matrix\n",
    "def create_adjacency_matrix(num_nodes=90, k=5):\n",
    "    adj = np.zeros((num_nodes, num_nodes), dtype=np.float32)\n",
    "    for i in range(num_nodes):\n",
    "        for j in range(max(0, i - k), min(num_nodes, i + k + 1)):\n",
    "            if i != j:\n",
    "                adj[i, j] = 1.0\n",
    "    np.fill_diagonal(adj, 1.0)\n",
    "    return adj\n",
    "\n"
   ],
   "id": "3f840e230e44cd40"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Preparing and Training GAT Model\n",
    "\n",
    "## **Steps**:\n",
    "\n",
    "### 1. **Adjacency Matrix**:\n",
    "   - Created using the function `create_adjacency_matrix(num_nodes=90, k=5)` for node connectivity.\n",
    "\n",
    "### 2. **Data Preparation**:\n",
    "   - `X_combined` as node features and `X_model_scaled` as targets.\n",
    "   - Replicated adjacency matrix for all samples.\n",
    "   - Split dataset into training and validation sets (90%-10%).\n",
    "\n",
    "### 3. **TensorFlow Tensors**:\n",
    "   - Converted data to **TensorFlow tensors** for model compatibility.\n",
    "\n",
    "### 4. **TensorFlow Datasets**:\n",
    "   - Created TensorFlow datasets for training and validation with shuffling and batching.\n",
    "\n",
    "### 5. **GAT Model Setup**:\n",
    "   - Defined GAT model with **Graph Attention Layers**, **Transformer Attention Blocks**, and **Dense Layers**.\n",
    "\n",
    "### 6. **Compilation & Training**:\n",
    "   - Compiled model with **Adam optimizer**, **mean squared error loss**, and **MAE metric**.\n",
    "   - Trained the model on the dataset for 100 epochs."
   ],
   "id": "89a2dae1f28fd9bb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "adjacency_matrix = create_adjacency_matrix(num_nodes=90, k=5)\n",
    "\n",
    "X = X_combined\n",
    "y = X_model_scaled\n",
    "\n",
    "# Replicate adjacency matrix for all samples\n",
    "num_samples = X.shape[0]\n",
    "A = np.tile(adjacency_matrix, (num_samples, 1, 1))\n",
    "\n",
    "# Split Dataset into Training and Validation Sets\n",
    "X_train, X_val, A_train, A_val, y_train, y_val = train_test_split(\n",
    "    X, A, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Convert to TensorFlow Tensors\n",
    "X_train = tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
    "X_val = tf.convert_to_tensor(X_val, dtype=tf.float32)\n",
    "A_train = tf.convert_to_tensor(A_train, dtype=tf.float32)\n",
    "A_val = tf.convert_to_tensor(A_val, dtype=tf.float32)\n",
    "y_train = tf.convert_to_tensor(y_train, dtype=tf.float32)\n",
    "y_val = tf.convert_to_tensor(y_val, dtype=tf.float32)\n",
    "\n",
    "buffer_size = X_train.shape[0]\n",
    "\n",
    "# Create TensorFlow datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(((X_train, A_train), y_train)).shuffle(buffer_size).batch(64).prefetch(tf.data.AUTOTUNE)\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices(((X_val, A_val), y_val)).batch(64).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "print(\"Training and validation datasets prepared.\")\n",
    "\n",
    "# Define and Train the GAT Model\n",
    "gat_model = GAT(num_heads=8, hidden_dim=64, output_dim=100, dropout_rate=0.01)\n",
    "\n",
    "# Compile the Model\n",
    "gat_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "    loss='mean_squared_error',\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "# Build the Model\n",
    "node_features_input = Input(shape=(90, 2), dtype=tf.float32)\n",
    "adjacency_input = Input(shape=(90, 90), dtype=tf.float32)\n",
    "gat_output = gat_model((node_features_input, adjacency_input))\n",
    "\n",
    "# Train the Model\n",
    "history = gat_model.fit(train_dataset, validation_data=val_dataset, epochs=100)\n"
   ],
   "id": "f7326d0a849f2ce5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
