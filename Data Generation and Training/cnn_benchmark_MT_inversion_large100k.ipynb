{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Benchmark for MT Inversion – 100 Layers, 100,000 Samples\n",
    "\n",
    "Configuration uses 90 frequencies, 100 depth outputs, Gaussian noise tests (1%, 3%, 5%), and a Conv1D architecture with final Dense(100)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3f698e",
   "metadata": {},
   "source": [
    "## Scientific Background: Magnetotellurics (MT) Inversion\n",
    "\n",
    "**What is Magnetotellurics?**\n",
    "Magnetotellurics is a geophysical method that uses natural electromagnetic fields from the Earth's magnetosphere and lightning to probe the electrical conductivity structure of the subsurface. MT measurements record time-varying electric (E) and magnetic (H) fields at the Earth's surface.\n",
    "\n",
    "**The MT Transfer Function:**\n",
    "The relationship between E and H fields is described by the impedance tensor Z(ω), where ω is the angular frequency:\n",
    "- E(ω) = Z(ω) · H(ω)\n",
    "\n",
    "**Observable Quantities:**\n",
    "From the impedance Z, we derive two key observables:\n",
    "1. **Apparent Resistivity (ρₐ)**: ρₐ = |Z|²/(μ₀ω), measured in Ohm·m\n",
    "2. **Phase (φ)**: φ = arg(Z), measured in degrees\n",
    "\n",
    "**The Inverse Problem:**\n",
    "The goal of MT inversion is to recover the true subsurface resistivity structure ρ(z) from the measured apparent resistivity and phase. This is a classic **ill-posed inverse problem** because:\n",
    "- Multiple resistivity models can produce similar surface measurements\n",
    "- Noise in measurements adds uncertainty\n",
    "- The problem is non-linear\n",
    "\n",
    "**CNN Approach:**\n",
    "This notebook uses Convolutional Neural Networks (CNNs) to solve the MT inverse problem by:\n",
    "1. Training on 100,000 synthetic forward models\n",
    "2. Learning the mapping from (ρₐ, φ) → ρ(z)\n",
    "3. Testing robustness against measurement noise (1%, 3%, 5%)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca385fbf",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# IMPORT LIBRARIES\n",
    "# ============================================================================\n",
    "\n",
    "import os, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Scikit-learn utilities for data preprocessing and splitting\n",
    "from sklearn.preprocessing import MinMaxScaler  # Scales features to [0,1] range\n",
    "from sklearn.model_selection import train_test_split  # Splits data into train/val/test sets\n",
    "from scipy.interpolate import make_interp_spline  # Creates smooth spline interpolation for resistivity profiles\n",
    "\n",
    "# TensorFlow and Keras for deep learning\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# ============================================================================\n",
    "# REPRODUCIBILITY SETUP\n",
    "# ============================================================================\n",
    "# Set random seeds for all libraries to ensure reproducible results across runs\n",
    "\n",
    "SEED = 42\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)  # Hash-based operations deterministic\n",
    "random.seed(SEED)  # Python's built-in random module\n",
    "np.random.seed(SEED)  # NumPy random number generation\n",
    "tf.random.set_seed(SEED)  # TensorFlow random operations\n",
    "\n",
    "# ============================================================================\n",
    "# GPU CONFIGURATION\n",
    "# ============================================================================\n",
    "# Configure TensorFlow to use GPU if available and enable dynamic memory growth\n",
    "# to prevent TensorFlow from allocating all GPU memory at once\n",
    "\n",
    "print(\"Num GPUs Available:\", len(tf.config.list_physical_devices('GPU')))\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            # Enable memory growth: TensorFlow allocates GPU memory as needed\n",
    "            # rather than pre-allocating all available GPU memory\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(\"Using GPU:\", gpus[0])\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "# ============================================================================\n",
    "# CUSTOM METRICS: CORRELATION COEFFICIENT\n",
    "# ============================================================================\n",
    "\n",
    "def corrcoef_batch(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute Pearson correlation coefficient averaged over a batch.\n",
    "    \n",
    "    This metric measures the linear relationship between predicted and true\n",
    "    resistivity profiles. R values range from -1 (perfect negative correlation)\n",
    "    to +1 (perfect positive correlation), with 0 indicating no correlation.\n",
    "    \n",
    "    For MT inversion, high R values (>0.9) indicate the model accurately\n",
    "    captures the overall shape and trends of resistivity profiles.\n",
    "    \n",
    "    Args:\n",
    "        y_true: True resistivity profiles, shape (batch_size, num_layers)\n",
    "        y_pred: Predicted resistivity profiles, shape (batch_size, num_layers)\n",
    "    \n",
    "    Returns:\n",
    "        Mean correlation coefficient across the batch (scalar)\n",
    "    \"\"\"\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    \n",
    "    # Compute mean for each sample (along depth dimension)\n",
    "    y_true_mean = tf.reduce_mean(y_true, axis=1, keepdims=True)\n",
    "    y_pred_mean = tf.reduce_mean(y_pred, axis=1, keepdims=True)\n",
    "    \n",
    "    # Pearson correlation formula: r = Σ((x-x̄)(y-ȳ)) / √(Σ(x-x̄)² · Σ(y-ȳ)²)\n",
    "    num = tf.reduce_sum((y_true - y_true_mean) * (y_pred - y_pred_mean), axis=1)\n",
    "    den = tf.sqrt(tf.reduce_sum((y_true - y_true_mean)**2, axis=1) * \n",
    "                  tf.reduce_sum((y_pred - y_pred_mean)**2, axis=1) + 1e-12)\n",
    "    r = num / (den + 1e-12)  # Added epsilon for numerical stability\n",
    "    \n",
    "    return tf.reduce_mean(r)  # Average correlation across batch\n",
    "\n",
    "def numpy_corrcoef(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    NumPy implementation of correlation coefficient for evaluation.\n",
    "    \n",
    "    Used during final model evaluation to compute correlation on CPU.\n",
    "    Computes correlation for each sample individually, then averages.\n",
    "    \n",
    "    Args:\n",
    "        y_true: True resistivity profiles (N_samples, num_layers)\n",
    "        y_pred: Predicted resistivity profiles (N_samples, num_layers)\n",
    "    \n",
    "    Returns:\n",
    "        Mean correlation coefficient (float)\n",
    "    \"\"\"\n",
    "    rs = []\n",
    "    for t, p in zip(y_true, y_pred):\n",
    "        t_mean = t.mean()\n",
    "        p_mean = p.mean()\n",
    "        num = np.sum((t - t_mean) * (p - p_mean))\n",
    "        den = np.sqrt(np.sum((t - t_mean)**2) * np.sum((p - p_mean)**2) + 1e-12)\n",
    "        rs.append(num / (den + 1e-12))\n",
    "    return float(np.mean(rs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d71c6a",
   "metadata": {},
   "source": [
    "### Forward Modeling Physics\n",
    "\n",
    "**Key Physical Principle - Maxwell's Equations:**\n",
    "MT forward modeling solves the 1D diffusion equation derived from Maxwell's equations in a layered medium:\n",
    "\n",
    "∂²E/∂z² = iωμ₀σE\n",
    "\n",
    "where:\n",
    "- σ = 1/ρ (conductivity = inverse of resistivity)\n",
    "- ω = 2π/T (angular frequency)\n",
    "- μ₀ = 4π×10⁻⁷ H/m (magnetic permeability of free space)\n",
    "\n",
    "**Recursive Impedance Algorithm:**\n",
    "The code implements Wait's recursion (1954) to efficiently compute impedance:\n",
    "1. Start at the bottom layer (half-space)\n",
    "2. Propagate impedance upward through each layer using:\n",
    "   - Wave number: K = √(σμ₀ωi)\n",
    "   - Layer response with hyperbolic tangent\n",
    "3. Surface impedance gives apparent resistivity and phase\n",
    "\n",
    "**Why 100,000 Samples?**\n",
    "Deep learning requires large datasets to:\n",
    "- Learn complex non-linear relationships\n",
    "- Generalize across diverse resistivity structures\n",
    "- Handle measurement noise robustly\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generation: Forward MT Modeling\n",
    "\n",
    "**Physical Parameters:**\n",
    "The forward problem calculates how electromagnetic waves propagate through a layered Earth based on:\n",
    "- **Skin depth (δ)**: The depth at which EM field amplitude decays to 1/e (~37%) of surface value\n",
    "  - δ ≈ 500√(ρT) meters, where ρ is resistivity (Ohm·m) and T is period (seconds)\n",
    "  - Longer periods penetrate deeper into the Earth\n",
    "\n",
    "**Frequency Range:**\n",
    "Using 90 logarithmically-spaced periods from 0.001s to 1000s:\n",
    "- Short periods (0.001-1s): Probe shallow depths (meters to hundreds of meters)\n",
    "- Long periods (1-1000s): Probe deep structures (kilometers to tens of kilometers)\n",
    "\n",
    "**Layer Discretization:**\n",
    "The subsurface is divided into 100 layers with exponentially increasing thickness:\n",
    "- Thin layers near surface for high resolution\n",
    "- Thick layers at depth where resolution decreases\n",
    "- Layer thicknesses scale with skin depth to capture physical behavior\n",
    "\n",
    "**Synthetic Data Generation:**\n",
    "1. Create smooth resistivity profiles using spline interpolation (realistic geological structures)\n",
    "2. Solve Maxwell's equations using recursive impedance formulation\n",
    "3. Generate apparent resistivity and phase curves\n",
    "4. Add realistic Gaussian noise to test robustness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc92656e",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92aba6d2",
   "metadata": {},
   "source": [
    "### Why Data Normalization?\n",
    "\n",
    "**MinMax Scaling to [0,1]:**\n",
    "MT data spans many orders of magnitude:\n",
    "- Resistivity: 1 to 10,000 Ohm·m (4 orders)\n",
    "- Phase: 0° to 90°\n",
    "- Depths: exponential spacing\n",
    "\n",
    "Normalization ensures:\n",
    "1. **Numerical stability**: Prevents gradient vanishing/explosion\n",
    "2. **Fair feature weighting**: ρₐ and φ contribute equally\n",
    "3. **Faster convergence**: Optimization works better on [0,1] scale\n",
    "4. **Better generalization**: Network learns relative patterns, not absolute values\n",
    "\n",
    "All predictions are inverse-transformed back to physical units for evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mu_0 = 4 * np.pi * 1e-7\n",
    "N_FREQ = 90\n",
    "periods = np.logspace(-3, 3, N_FREQ)\n",
    "resistivity_range = [1.0, 1e4]\n",
    "rho_ref = 100.0\n",
    "\n",
    "min_period = periods.min()\n",
    "max_period = periods.max()\n",
    "min_skin_depth = 500 * np.sqrt(rho_ref * min_period) / 4\n",
    "max_skin_depth = 500 * np.sqrt(rho_ref * max_period)\n",
    "\n",
    "M_OUTPUT = 100\n",
    "r = (max_skin_depth / min_skin_depth) ** (1.0 / (M_OUTPUT - 1))\n",
    "layer_thicknesses = np.array([min_skin_depth * (r ** i) for i in range(M_OUTPUT)])\n",
    "num_layers = len(layer_thicknesses)\n",
    "print(\"Layers:\", num_layers)\n",
    "\n",
    "def generate_smooth_resistivity_profile(num_points, resistivity_range, n_ctrl=10, seed=None):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    depth_points = np.linspace(0, num_points - 1, num=n_ctrl)\n",
    "    resistivity_points = rng.uniform(resistivity_range[0], resistivity_range[1], size=n_ctrl)\n",
    "    spline = make_interp_spline(depth_points, resistivity_points, k=3)\n",
    "    smooth_resistivities = spline(np.linspace(0, num_points - 1, num_points))\n",
    "    smooth_resistivities = np.clip(smooth_resistivities, resistivity_range[0], resistivity_range[1])\n",
    "    return smooth_resistivities\n",
    "\n",
    "def compute_apparent_resistivity_and_phase(thicknesses, conductivities, periods):\n",
    "    apparent_resistivity = []\n",
    "    phase = []\n",
    "    for T in periods:\n",
    "        omega = 2 * np.pi / T\n",
    "        cns = np.zeros(len(conductivities), dtype=complex)\n",
    "        cns[-1] = 1 / np.sqrt(mu_0 * omega * conductivities[-1] * 1j)\n",
    "        for j in reversed(range(len(thicknesses))):\n",
    "            K = np.sqrt(conductivities[j] * mu_0 * omega * 1j)\n",
    "            layer_thickness = thicknesses[j] if j < len(thicknesses) - 1 else np.inf\n",
    "            if j + 1 < len(cns):\n",
    "                cns[j] = (1 / K) * ((K * cns[j + 1] + np.tanh(K * layer_thickness)) / (1 + K * cns[j + 1] * np.tanh(K * layer_thickness)))\n",
    "        Z = cns[0]\n",
    "        rho_apparent = np.abs(Z) ** 2 * (mu_0 * omega)\n",
    "        phi = np.degrees(np.angle(Z)) + 90.0\n",
    "        apparent_resistivity.append(rho_apparent)\n",
    "        phase.append(phi)\n",
    "    return np.array(apparent_resistivity), np.array(phase)\n",
    "\n",
    "def resample_to_M(profile_native, M=M_OUTPUT):\n",
    "    x_native = np.linspace(0, 1, num=len(profile_native))\n",
    "    x_target = np.linspace(0, 1, num=M)\n",
    "    return np.interp(x_target, x_native, profile_native)\n",
    "\n",
    "N_STATIONS = 100000\n",
    "\n",
    "X_clean_list = []\n",
    "y_list = []\n",
    "rng = np.random.default_rng(SEED)\n",
    "\n",
    "for _ in range(N_STATIONS):\n",
    "    resistivities_native = generate_smooth_resistivity_profile(M_OUTPUT, resistivity_range)\n",
    "    conductivities_native = 1.0 / resistivities_native\n",
    "    rho_a, phi = compute_apparent_resistivity_and_phase(layer_thicknesses, conductivities_native, periods)\n",
    "    resistivities_M = resample_to_M(resistivities_native, M_OUTPUT)\n",
    "    X_pair = np.stack([rho_a, phi], axis=1)\n",
    "    X_clean_list.append(X_pair.astype(np.float32))\n",
    "    y_list.append(resistivities_M.astype(np.float32))\n",
    "\n",
    "X_clean = np.stack(X_clean_list, axis=0)\n",
    "y = np.stack(y_list, axis=0)\n",
    "print(\"X_clean:\", X_clean.shape, \"| y:\", y.shape)\n",
    "\n",
    "X_rho = X_clean[:, :, 0].reshape(-1, 1)\n",
    "X_phi = X_clean[:, :, 1].reshape(-1, 1)\n",
    "y_vec = y.reshape(-1, 1)\n",
    "\n",
    "scaler_rho = MinMaxScaler()\n",
    "scaler_phi = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "\n",
    "X_rho_scaled_all = scaler_rho.fit_transform(X_rho).reshape(X_clean.shape[0], X_clean.shape[1])\n",
    "X_phi_scaled_all = scaler_phi.fit_transform(X_phi).reshape(X_clean.shape[0], X_clean.shape[1])\n",
    "y_scaled_all = scaler_y.fit_transform(y_vec).reshape(y.shape[0], y.shape[1])\n",
    "\n",
    "X_scaled = np.stack([X_rho_scaled_all, X_phi_scaled_all], axis=2)\n",
    "y_scaled = y_scaled_all\n",
    "\n",
    "def add_gaussian_noise(x, std):\n",
    "    noisy = x + np.random.normal(loc=0.0, scale=std, size=x.shape).astype(np.float32)\n",
    "    return np.clip(noisy, 0.0, 1.0)\n",
    "\n",
    "X_noise_1 = add_gaussian_noise(X_scaled, 0.01)\n",
    "X_noise_3 = add_gaussian_noise(X_scaled, 0.03)\n",
    "X_noise_5 = add_gaussian_noise(X_scaled, 0.05)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Splitting Strategy\n",
    "\n",
    "**Train-Validation-Test Split:**\n",
    "- **Training set (64%)**: 64,000 samples to learn the inverse mapping\n",
    "- **Validation set (16%)**: 16,000 samples for hyperparameter tuning and preventing overfitting\n",
    "- **Test set (20%)**: 20,000 samples for final unbiased performance evaluation\n",
    "\n",
    "**Noise Augmentation for Robustness:**\n",
    "Real MT measurements contain noise from:\n",
    "- Atmospheric electromagnetic interference\n",
    "- Cultural noise (power lines, railways)\n",
    "- Instrument drift and digitization errors\n",
    "\n",
    "We test three noise levels on scaled data [0,1]:\n",
    "- **1% noise (σ=0.01)**: Excellent field conditions\n",
    "- **3% noise (σ=0.03)**: Typical field conditions\n",
    "- **5% noise (σ=0.05)**: Poor field conditions or distant/weak signals\n",
    "\n",
    "Gaussian noise is added to both apparent resistivity and phase after normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3c0d14",
   "metadata": {},
   "source": [
    "### Deep Learning vs Traditional MT Inversion\n",
    "\n",
    "**Traditional Methods:**\n",
    "- **Occam's inversion**: Seeks smoothest model fitting data\n",
    "- **Iterative linearization**: Slow, requires good initial guess\n",
    "- **Computationally expensive**: Hours to days per inversion\n",
    "\n",
    "**CNN Advantages:**\n",
    "1. **Speed**: Real-time inversion (<1ms per station after training)\n",
    "2. **No initial model needed**: Direct mapping from data to structure\n",
    "3. **Handles noise naturally**: Learned during training on noisy data\n",
    "4. **Batch processing**: Can invert thousands of stations simultaneously\n",
    "\n",
    "**Trade-offs:**\n",
    "- Requires large training dataset (expensive forward modeling once)\n",
    "- Limited to distribution of training models\n",
    "- Less interpretable than physics-based inversions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f211aa8",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(\n",
    "    X_scaled, y_scaled, test_size=0.2, random_state=SEED, shuffle=True)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_trainval, y_trainval, test_size=0.2, random_state=SEED, shuffle=True)\n",
    "\n",
    "X_test_n1 = X_noise_1[X_scaled.shape[0] - X_test.shape[0]:]\n",
    "X_test_n3 = X_noise_3[X_scaled.shape[0] - X_test.shape[0]:]\n",
    "X_test_n5 = X_noise_5[X_scaled.shape[0] - X_test.shape[0]:]\n",
    "\n",
    "print(\"Train:\", X_train.shape, y_train.shape)\n",
    "print(\"Val:\", X_val.shape, y_val.shape)\n",
    "print(\"Test:\", X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33fe6b1",
   "metadata": {},
   "source": [
    "### Training on Large Dataset\n",
    "\n",
    "**Why 100,000 Samples?**\n",
    "- MT inverse problem is highly non-unique (many models fit data)\n",
    "- Need to explore vast resistivity model space\n",
    "- CNN must learn which features distinguish different structures\n",
    "- Rare but important geological structures need representation\n",
    "\n",
    "**GPU Acceleration:**\n",
    "- Training on 100k samples would take days on CPU\n",
    "- GPU parallelizes convolution operations across:\n",
    "  - Batch dimension (128 samples at once)\n",
    "  - Filter operations (thousands of multiplications)\n",
    "  - Frequency channels\n",
    "- Typical speedup: 10-50x over CPU\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Architecture for MT Inversion\n",
    "\n",
    "**Why CNNs for MT Data?**\n",
    "Convolutional Neural Networks are ideal because:\n",
    "1. **Local patterns**: Nearby frequencies are physically related (skin depth smoothly varies)\n",
    "2. **Translation invariance**: Same patterns at different frequency bands\n",
    "3. **Hierarchical learning**: Early layers detect local features, deeper layers integrate global structure\n",
    "4. **Parameter efficiency**: Shared weights across frequency reduce overfitting\n",
    "\n",
    "**Architecture Design:**\n",
    "```\n",
    "Input: (90 frequencies, 2 features [ρₐ, φ])\n",
    "├─ Conv1D(32 filters, kernel=5) → Learn local frequency patterns\n",
    "├─ MaxPool(2) → Downsample, extract dominant features\n",
    "├─ Conv1D(64 filters, kernel=3) → Learn mid-scale patterns\n",
    "├─ MaxPool(2) → Further abstraction\n",
    "├─ Conv1D(128 filters, kernel=3) → Learn global frequency relationships\n",
    "├─ MaxPool(2) → Final feature extraction\n",
    "├─ Flatten → Convert to 1D feature vector\n",
    "├─ Dense(128) + Dropout(0.05) → High-level reasoning\n",
    "└─ Dense(100) → Output: 100-layer resistivity profile\n",
    "```\n",
    "\n",
    "**Regularization:**\n",
    "- **L2 regularization (1e-5)**: Prevents overfitting by penalizing large weights\n",
    "- **Dropout (5%)**: Randomly disables neurons during training for robustness\n",
    "- **Early stopping**: Halts training when validation loss stops improving\n",
    "\n",
    "**Loss Function:**\n",
    "- **MSE (Mean Squared Error)**: Measures average squared difference in resistivity\n",
    "- **Correlation coefficient (R)**: Measures how well predicted shape matches true profile (more geophysically meaningful)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8b58be",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_cnn(input_shape, l2_reg=1e-5, dropout_rate=0.05):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = Conv1D(filters=32, kernel_size=5, activation='relu', padding='same')(inputs)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    x = Conv1D(filters=64, kernel_size=3, activation='relu', padding='same')(x)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    x = Conv1D(filters=128, kernel_size=3, activation='relu', padding='same')(x)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(128, activation='relu', kernel_regularizer=regularizers.l2(l2_reg))(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    outputs = Dense(100, activation='linear', kernel_regularizer=regularizers.l2(l2_reg))(x)\n",
    "    model = Model(inputs, outputs, name='CNN_MT_Inversion')\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "                  loss='mse',\n",
    "                  metrics=[corrcoef_batch])\n",
    "    return model\n",
    "\n",
    "cnn = build_cnn(input_shape=X_train.shape[1:])\n",
    "cnn.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "**Training Strategy:**\n",
    "The network learns by minimizing the difference between predicted and true resistivity profiles over 64,000 training examples.\n",
    "\n",
    "**Callbacks:**\n",
    "1. **Early Stopping (patience=10)**: \n",
    "   - Monitors validation loss\n",
    "   - Stops if no improvement for 10 epochs\n",
    "   - Restores best weights to prevent overfitting\n",
    "   - Prevents wasting computation on diminishing returns\n",
    "\n",
    "2. **ReduceLROnPlateau (patience=5)**:\n",
    "   - Reduces learning rate by 50% when validation loss plateaus\n",
    "   - Helps escape local minima\n",
    "   - Fine-tunes weights in later epochs\n",
    "   - Minimum LR: 1e-5\n",
    "\n",
    "**Batch Size = 128:**\n",
    "- Balances memory usage and convergence speed\n",
    "- Provides stable gradient estimates\n",
    "- Allows parallel GPU processing\n",
    "\n",
    "**Expected Behavior:**\n",
    "- Training loss should decrease steadily\n",
    "- Validation loss should follow training loss closely\n",
    "- Correlation coefficient R should approach 0.95+ for clean data\n",
    "- Typical convergence: 30-50 epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402c49ad",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(patience=10, monitor='val_loss', restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor='val_loss', patience=5, factor=0.5, min_lr=1e-5, verbose=1),\n",
    "]\n",
    "history = cnn.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=100,\n",
    "    batch_size=128,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation and Noise Robustness\n",
    "\n",
    "**Evaluation Metrics:**\n",
    "\n",
    "1. **Mean Squared Error (MSE)**:\n",
    "   - MSE = (1/N)Σ(ρ_true - ρ_pred)²\n",
    "   - Measures average squared difference in resistivity (Ohm·m)²\n",
    "   - Sensitive to outliers\n",
    "   - Lower is better\n",
    "\n",
    "2. **Correlation Coefficient (R)**:\n",
    "   - R = Pearson correlation between true and predicted profiles\n",
    "   - Ranges from -1 to +1 (perfect positive correlation)\n",
    "   - **R > 0.9**: Excellent inversion quality\n",
    "   - **R > 0.8**: Good for practical applications\n",
    "   - **R < 0.7**: Poor, unreliable inversion\n",
    "   - More geophysically meaningful than MSE\n",
    "\n",
    "**Noise Robustness Test:**\n",
    "Real MT data always contains noise. We evaluate on:\n",
    "1. **Clean data**: Best-case performance baseline\n",
    "2. **1% noise**: High-quality field data\n",
    "3. **3% noise**: Typical field conditions\n",
    "4. **5% noise**: Challenging conditions (requires noise filtering)\n",
    "\n",
    "**Expected Results:**\n",
    "- Clean data: R ≈ 0.95-0.98, very low MSE\n",
    "- Noise degrades both metrics progressively\n",
    "- Good model should maintain R > 0.85 even at 5% noise\n",
    "- MSE increases quadratically with noise level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def evaluate_model(model, X, y_true_scaled, scaler_y):\n",
    "    \"\"\"\n",
    "    Evaluate CNN inversion on test data with specified noise level.\n",
    "    \n",
    "    Steps:\n",
    "    1. Predict normalized resistivity profiles\n",
    "    2. Inverse transform to physical units (Ohm·m)\n",
    "    3. Compute MSE and correlation coefficient\n",
    "    \n",
    "    Returns metrics in physical units for interpretability.\n",
    "    \"\"\"\n",
    "    y_pred_scaled = model.predict(X, verbose=0)\n",
    "    y_true = scaler_y.inverse_transform(y_true_scaled)\n",
    "    y_pred = scaler_y.inverse_transform(y_pred_scaled)\n",
    "    mse = mean_squared_error(y_true.reshape(-1), y_pred.reshape(-1))\n",
    "    R = numpy_corrcoef(y_true, y_pred)\n",
    "    return mse, R, y_true, y_pred\n",
    "\n",
    "# Test on clean and progressively noisier data\n",
    "results = []\n",
    "for name, X_eval in [('Clean', X_test), ('Noise 1%', X_test_n1), ('Noise 3%', X_test_n3), ('Noise 5%', X_test_n5)]:\n",
    "    mse, R, y_true_inv, y_pred_inv = evaluate_model(cnn, X_eval, y_test, scaler_y)\n",
    "    results.append((name, mse, R))\n",
    "\n",
    "df_results = pd.DataFrame(results, columns=['Condition', 'MSE', 'R'])\n",
    "df_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Visualization\n",
    "\n",
    "**Plot 1: Training History**\n",
    "- Shows how loss (MSE) decreases during training\n",
    "- Validation loss should track training loss (no overfitting)\n",
    "- Early stopping prevents divergence\n",
    "\n",
    "**Plot 2: Correlation Evolution**\n",
    "- Shows R metric improving during training\n",
    "- Should plateau near 0.95+ for good inversion\n",
    "- Indicates model is learning physical relationships\n",
    "\n",
    "**Plot 3: Individual Station Examples**\n",
    "- Compares true vs predicted resistivity profiles\n",
    "- X-axis: Normalized depth (0=shallow, 1=deep)\n",
    "- Y-axis: Resistivity in Ohm·m (log scale expected)\n",
    "- Good predictions follow true profile shape and amplitude\n",
    "\n",
    "**Plot 4: Noise Robustness**\n",
    "- MSE vs noise level\n",
    "- Should show gradual degradation, not collapse\n",
    "- Demonstrates model generalization\n",
    "\n",
    "**Geophysical Interpretation:**\n",
    "- **Conductive layers** (low ρ): Sediments, water-saturated zones, clay-rich rocks\n",
    "- **Resistive layers** (high ρ): Crystalline basement, dry rocks, carbonates\n",
    "- **Sharp boundaries**: Geological contacts (unconformities, faults)\n",
    "- **Smooth transitions**: Gradational lithology changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot 1: Loss convergence shows optimization success\n",
    "plt.figure(figsize=(8, 5), dpi=150)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.xlabel('Epoch'); plt.ylabel('MSE'); plt.title('Training/Validation Loss'); plt.legend(); plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot 2: Correlation coefficient tracks inversion quality\n",
    "tr = history.history.get('corrcoef_batch', None)\n",
    "va = history.history.get('val_corrcoef_batch', None)\n",
    "if tr is not None and va is not None:\n",
    "    plt.figure(figsize=(8, 5), dpi=150)\n",
    "    plt.plot(tr, label='Train R')\n",
    "    plt.plot(va, label='Val R')\n",
    "    plt.xlabel('Epoch'); plt.ylabel('R'); plt.title('Training/Validation Correlation'); plt.legend(); plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot 3: Individual station inversions demonstrate accuracy\n",
    "n_show = 5\n",
    "idxs = np.random.choice(X_test.shape[0], size=n_show, replace=False)\n",
    "y_pred_scaled = cnn.predict(X_test[idxs], verbose=0)\n",
    "y_pred = scaler_y.inverse_transform(y_pred_scaled)\n",
    "y_true = scaler_y.inverse_transform(y_test[idxs])\n",
    "depths = np.linspace(0, 1, 100)\n",
    "\n",
    "fig, axes = plt.subplots(n_show, 1, figsize=(7, 2.5*n_show), dpi=150, sharex=True)\n",
    "if n_show == 1: axes = [axes]\n",
    "for ax, i in zip(axes, range(n_show)):\n",
    "    ax.plot(depths, y_true[i], label='True', lw=2)\n",
    "    ax.plot(depths, y_pred[i], label='Pred', lw=1.5)\n",
    "    ax.set_ylabel('Resistivity (Ohm·m)')\n",
    "    ax.set_title(f'Test Station Example {i+1}')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "axes[-1].set_xlabel('Normalized Depth')\n",
    "axes[0].legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot 4: Noise robustness test - critical for field applications\n",
    "plt.figure(figsize=(6,4), dpi=150)\n",
    "plt.plot(df_results['Condition'], df_results['MSE'], marker='o')\n",
    "plt.xlabel('Condition'); plt.ylabel('MSE'); plt.title('Noise Robustness (CNN)'); plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "df_results\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
