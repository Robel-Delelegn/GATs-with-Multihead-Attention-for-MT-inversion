# GATs-with-Multihead-Attention-for-MT-inversion
Our proposed GAT-Attention model provides a more physically informed and computationally efficient approach to MT inversion, addressing the key shortcomings of existing machine learning methods
